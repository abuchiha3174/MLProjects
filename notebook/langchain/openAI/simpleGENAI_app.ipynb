{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMPLE GEN Ai APP using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1ae108e2a20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Ingestion from a website that we need to scrape the data from\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(\"https://python.langchain.com/docs/tutorials/chatbot/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a ChatbotOn this pageBuild a Chatbot\\nnoteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nOverview‚Äã\\nWe\\'ll go over an example of how to design and implement an LLM-powered chatbot.\\nThis chatbot will be able to have a conversation and remember previous interactions with a chat model.\\nNote that this chatbot that we build will only use the language model to have a conversation.\\nThere are several other related concepts that you may be looking for:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nThis tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\\nSetup‚Äã\\nJupyter Notebook‚Äã\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation‚Äã\\nFor this tutorial we will need langchain-core and langgraph. This guide requires langgraph >= 0.2.28.\\n\\nPipCondapip install langchain-core langgraph>0.2.27conda install langchain-core langgraph>0.2.27 -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith‚Äã\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nQuickstart‚Äã\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!\\n\\nSelect chat model:OpenAI‚ñæOpenAIAnthropicAzureGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIpip install -qU \"langchain[openai]\"import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I\\'m Bob\")])API Reference:HumanMessage\\nAIMessage(content=\\'Hi Bob! How can I assist you today?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 10, \\'prompt_tokens\\': 11, \\'total_tokens\\': 21, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 10, \\'total_tokens\\': 21, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nThe model on its own does not have any concept of state. For example, if you ask a followup question:\\nmodel.invoke([HumanMessage(content=\"What\\'s my name?\")])\\nAIMessage(content=\"I\\'m sorry, but I don\\'t have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 34, \\'prompt_tokens\\': 11, \\'total_tokens\\': 45, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-a2d13a18-7022-4784-b54f-f85c097d1075-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 34, \\'total_tokens\\': 45, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nLet\\'s take a look at the example LangSmith trace\\nWe can see that it doesn\\'t take the previous conversation turn into context, and cannot answer the question.\\nThis makes for a terrible chatbot experience!\\nTo get around this, we need to pass the entire conversation history into the model. Let\\'s see what happens when we do that:\\nfrom langchain_core.messages import AIMessagemodel.invoke(    [        HumanMessage(content=\"Hi! I\\'m Bob\"),        AIMessage(content=\"Hello Bob! How can I assist you today?\"),        HumanMessage(content=\"What\\'s my name?\"),    ])API Reference:AIMessage\\nAIMessage(content=\\'Your name is Bob! How can I help you today, Bob?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 14, \\'prompt_tokens\\': 33, \\'total_tokens\\': 47, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-34bcccb3-446e-42f2-b1de-52c09936c02c-0\\', usage_metadata={\\'input_tokens\\': 33, \\'output_tokens\\': 14, \\'total_tokens\\': 47, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nAnd now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot\\'s ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence‚Äã\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    response = model.invoke(state[\"messages\"])    return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:MemorySaver | StateGraph\\nWe now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\\nThis enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\\nWe can then invoke the application:\\nquery = \"Hi! I\\'m Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()  # output contains all messages in state\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Hi Bob! How can I assist you today?\\nquery = \"What\\'s my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob! How can I help you today, Bob?\\nGreat! Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh.\\nconfig = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================I\\'m sorry, but I don\\'t have access to personal information about you unless you\\'ve shared it in this conversation. How can I assist you today?\\nHowever, we can always go back to the original conversation (since we are persisting it in a database)\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob. What would you like to discuss today?\\nThis is how we can support a chatbot having conversations with many users!\\ntipFor async support, update the call_model node to be an async function and use .ainvoke when invoking the application:# Async function for node:async def call_model(state: MessagesState):    response = await model.ainvoke(state[\"messages\"])    return {\"messages\": response}# Define graph as before:workflow = StateGraph(state_schema=MessagesState)workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)app = workflow.compile(checkpointer=MemorySaver())# Async invocation:output = await app.ainvoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\nRight now, all we\\'ve done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\\nPrompt templates‚Äã\\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let\\'s now make that a bit more complicated. First, let\\'s add in a system message with some custom instructions (but still taking messages as input). Next, we\\'ll add in more input besides just the messages.\\nTo add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You talk like a pirate. Answer all questions to the best of your ability.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])API Reference:ChatPromptTemplate | MessagesPlaceholder\\nWe can now update our application to incorporate this template:\\nworkflow = StateGraph(state_schema=MessagesState)def call_model(state: MessagesState):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": response}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nWe invoke the application in the same way:\\nconfig = {\"configurable\": {\"thread_id\": \"abc345\"}}query = \"Hi! I\\'m Jim.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ahoy there, Jim! What brings ye to these waters today? Be ye seekin\\' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\\nAwesome! Let\\'s now make our prompt a little bit more complicated. Let\\'s assume that the prompt template now looks something like this:\\nprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])\\nNote that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application\\'s state to reflect this:\\nfrom typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict):    messages: Annotated[Sequence[BaseMessage], add_messages]    language: strworkflow = StateGraph(state_schema=State)def call_model(state: State):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:BaseMessage | add_messages\\nconfig = {\"configurable\": {\"thread_id\": \"abc456\"}}query = \"Hi! I\\'m Bob.\"language = \"Spanish\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================¬°Hola, Bob! ¬øC√≥mo puedo ayudarte hoy?\\nNote that the entire state is persisted, so we can omit parameters like language if no changes are desired:\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Tu nombre es Bob. ¬øHay algo m√°s en lo que pueda ayudarte?\\nTo help you understand what\\'s happening internally, check out this LangSmith trace.\\nManaging Conversation History‚Äã\\nOne important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\\nImportantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\\nWe can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class.\\nLangChain comes with a few built-in helpers for managing a list of messages. In this case we\\'ll use the trim_messages helper to reduce how many messages we\\'re sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:\\nfrom langchain_core.messages import SystemMessage, trim_messagestrimmer = trim_messages(    max_tokens=65,    strategy=\"last\",    token_counter=model,    include_system=True,    allow_partial=False,    start_on=\"human\",)messages = [    SystemMessage(content=\"you\\'re a good assistant\"),    HumanMessage(content=\"hi! I\\'m bob\"),    AIMessage(content=\"hi!\"),    HumanMessage(content=\"I like vanilla ice cream\"),    AIMessage(content=\"nice\"),    HumanMessage(content=\"whats 2 + 2\"),    AIMessage(content=\"4\"),    HumanMessage(content=\"thanks\"),    AIMessage(content=\"no problem!\"),    HumanMessage(content=\"having fun?\"),    AIMessage(content=\"yes!\"),]trimmer.invoke(messages)API Reference:SystemMessage | trim_messages\\n[SystemMessage(content=\"you\\'re a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'whats 2 + 2\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'4\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'thanks\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'no problem!\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'having fun?\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'yes!\\', additional_kwargs={}, response_metadata={})]\\nTo  use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\\nworkflow = StateGraph(state_schema=State)def call_model(state: State):    trimmed_messages = trimmer.invoke(state[\"messages\"])    prompt = prompt_template.invoke(        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}    )    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nNow if we try asking the model our name, it won\\'t know it since we trimmed that part of the chat history:\\nconfig = {\"configurable\": {\"thread_id\": \"abc567\"}}query = \"What is my name?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================I don\\'t know your name. You haven\\'t told me yet!\\nBut if we ask about information that is within the last few messages, it remembers:\\nconfig = {\"configurable\": {\"thread_id\": \"abc678\"}}query = \"What math problem did I ask?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================You asked what 2 + 2 equals.\\nIf you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.\\nStreaming‚Äã\\nNow we\\'ve got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\\nIt\\'s actually super easy to do this!\\nBy default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:\\nconfig = {\"configurable\": {\"thread_id\": \"abc789\"}}query = \"Hi I\\'m Todd, please tell me a joke.\"language = \"English\"input_messages = [HumanMessage(query)]for chunk, metadata in app.stream(    {\"messages\": input_messages, \"language\": language},    config,    stream_mode=\"messages\",):    if isinstance(chunk, AIMessage):  # Filter to just model responses        print(chunk.content, end=\"|\")\\n|Hi| Todd|!| Here|‚Äôs| a| joke| for| you|:|Why| don|‚Äôt| skeleton|s| fight| each| other|?|Because| they| don|‚Äôt| have| the| guts|!||\\nNext Steps‚Äã\\nNow that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\\n\\nConversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nIf you want to dive deeper on specifics, some things worth checking out are:\\n\\nStreaming: streaming is crucial for chat applications\\nHow to add message history: for a deeper dive into all things related to message history\\nHow to manage large message history: more techniques for managing a large chat history\\nLangGraph main docs: for more detail on building with LangGraph\\nEdit this pageWas this page helpful?PreviousBuild a simple LLM application with chat models and prompt templatesNextBuild a Retrieval Augmented Generation (RAG) App: Part 2OverviewSetupJupyter NotebookInstallationLangSmithQuickstartMessage persistencePrompt templatesManaging Conversation HistoryStreamingNext StepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a ChatbotOn'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a ChatbotOn this pageBuild a Chatbot'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"noteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nOverview‚Äã\\nWe'll go over an example of how to design and implement an LLM-powered chatbot.\\nThis chatbot will be able to have a conversation and remember previous interactions with a chat model.\\nNote that this chatbot that we build will only use the language model to have a conversation.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='This chatbot will be able to have a conversation and remember previous interactions with a chat model.\\nNote that this chatbot that we build will only use the language model to have a conversation.\\nThere are several other related concepts that you may be looking for:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='Conversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nThis tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\\nSetup‚Äã\\nJupyter Notebook‚Äã\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation‚Äã\\nFor this tutorial we will need langchain-core and langgraph. This guide requires langgraph >= 0.2.28.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='PipCondapip install langchain-core langgraph>0.2.27conda install langchain-core langgraph>0.2.27 -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith‚Äã\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nQuickstart‚Äã'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='Or, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nQuickstart‚Äã\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='Select chat model:OpenAI‚ñæOpenAIAnthropicAzureGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIpip install -qU \"langchain[openai]\"import getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I\\'m Bob\")])API Reference:HumanMessage'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='from langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hi! I\\'m Bob\")])API Reference:HumanMessage\\nAIMessage(content=\\'Hi Bob! How can I assist you today?\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 10, \\'prompt_tokens\\': 11, \\'total_tokens\\': 21, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 10, \\'total_tokens\\': 21, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nThe model on its own does not have any concept of state. For example, if you ask a followup question:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='The model on its own does not have any concept of state. For example, if you ask a followup question:\\nmodel.invoke([HumanMessage(content=\"What\\'s my name?\")])'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='model.invoke([HumanMessage(content=\"What\\'s my name?\")])\\nAIMessage(content=\"I\\'m sorry, but I don\\'t have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 34, \\'prompt_tokens\\': 11, \\'total_tokens\\': 45, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-a2d13a18-7022-4784-b54f-f85c097d1075-0\\', usage_metadata={\\'input_tokens\\': 11, \\'output_tokens\\': 34, \\'total_tokens\\': 45, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\nLet\\'s take a look at the example LangSmith trace'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='Let\\'s take a look at the example LangSmith trace\\nWe can see that it doesn\\'t take the previous conversation turn into context, and cannot answer the question.\\nThis makes for a terrible chatbot experience!\\nTo get around this, we need to pass the entire conversation history into the model. Let\\'s see what happens when we do that:\\nfrom langchain_core.messages import AIMessagemodel.invoke(    [        HumanMessage(content=\"Hi! I\\'m Bob\"),        AIMessage(content=\"Hello Bob! How can I assist you today?\"),        HumanMessage(content=\"What\\'s my name?\"),    ])API Reference:AIMessage'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"AIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\\nAnd now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot's ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence‚Äã\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"And now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot's ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence‚Äã\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='LangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\\nfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.graph import START, MessagesState, StateGraph# Define a new graphworkflow = StateGraph(state_schema=MessagesState)# Define the function that calls the modeldef call_model(state: MessagesState):    response = model.invoke(state[\"messages\"])    return {\"messages\": response}# Define the (single) node in the graphworkflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)# Add memorymemory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:MemorySaver | StateGraph\\nWe now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='config = {\"configurable\": {\"thread_id\": \"abc123\"}}\\nThis enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\\nWe can then invoke the application:\\nquery = \"Hi! I\\'m Bob.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()  # output contains all messages in state\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Hi Bob! How can I assist you today?\\nquery = \"What\\'s my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob! How can I help you today, Bob?'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob! How can I help you today, Bob?\\nGreat! Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh.\\nconfig = {\"configurable\": {\"thread_id\": \"abc234\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================I\\'m sorry, but I don\\'t have access to personal information about you unless you\\'ve shared it in this conversation. How can I assist you today?\\nHowever, we can always go back to the original conversation (since we are persisting it in a database)\\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='config = {\"configurable\": {\"thread_id\": \"abc123\"}}input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Your name is Bob. What would you like to discuss today?\\nThis is how we can support a chatbot having conversations with many users!\\ntipFor async support, update the call_model node to be an async function and use .ainvoke when invoking the application:# Async function for node:async def call_model(state: MessagesState):    response = await model.ainvoke(state[\"messages\"])    return {\"messages\": response}# Define graph as before:workflow = StateGraph(state_schema=MessagesState)workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)app = workflow.compile(checkpointer=MemorySaver())# Async invocation:output = await app.ainvoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"Right now, all we've done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\\nPrompt templates‚Äã\\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\\nTo add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='To add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You talk like a pirate. Answer all questions to the best of your ability.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])API Reference:ChatPromptTemplate | MessagesPlaceholder\\nWe can now update our application to incorporate this template:\\nworkflow = StateGraph(state_schema=MessagesState)def call_model(state: MessagesState):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": response}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nWe invoke the application in the same way:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='We invoke the application in the same way:\\nconfig = {\"configurable\": {\"thread_id\": \"abc345\"}}query = \"Hi! I\\'m Jim.\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ahoy there, Jim! What brings ye to these waters today? Be ye seekin\\' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke({\"messages\": input_messages}, config)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\\nAwesome! Let\\'s now make our prompt a little bit more complicated. Let\\'s assume that the prompt template now looks something like this:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='Awesome! Let\\'s now make our prompt a little bit more complicated. Let\\'s assume that the prompt template now looks something like this:\\nprompt_template = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",        ),        MessagesPlaceholder(variable_name=\"messages\"),    ])\\nNote that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application\\'s state to reflect this:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='Note that we have added a new language input to the prompt. Our application now has two parameters-- the input messages and language. We should update our application\\'s state to reflect this:\\nfrom typing import Sequencefrom langchain_core.messages import BaseMessagefrom langgraph.graph.message import add_messagesfrom typing_extensions import Annotated, TypedDictclass State(TypedDict):    messages: Annotated[Sequence[BaseMessage], add_messages]    language: strworkflow = StateGraph(state_schema=State)def call_model(state: State):    prompt = prompt_template.invoke(state)    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)API Reference:BaseMessage | add_messages'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='config = {\"configurable\": {\"thread_id\": \"abc456\"}}query = \"Hi! I\\'m Bob.\"language = \"Spanish\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================¬°Hola, Bob! ¬øC√≥mo puedo ayudarte hoy?\\nNote that the entire state is persisted, so we can omit parameters like language if no changes are desired:\\nquery = \"What is my name?\"input_messages = [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================Tu nombre es Bob. ¬øHay algo m√°s en lo que pueda ayudarte?\\nTo help you understand what\\'s happening internally, check out this LangSmith trace.\\nManaging Conversation History‚Äã'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"To help you understand what's happening internally, check out this LangSmith trace.\\nManaging Conversation History‚Äã\\nOne important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\\nImportantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\\nWe can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"We can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the Message History class.\\nLangChain comes with a few built-in helpers for managing a list of messages. In this case we'll use the trim_messages helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='from langchain_core.messages import SystemMessage, trim_messagestrimmer = trim_messages(    max_tokens=65,    strategy=\"last\",    token_counter=model,    include_system=True,    allow_partial=False,    start_on=\"human\",)messages = [    SystemMessage(content=\"you\\'re a good assistant\"),    HumanMessage(content=\"hi! I\\'m bob\"),    AIMessage(content=\"hi!\"),    HumanMessage(content=\"I like vanilla ice cream\"),    AIMessage(content=\"nice\"),    HumanMessage(content=\"whats 2 + 2\"),    AIMessage(content=\"4\"),    HumanMessage(content=\"thanks\"),    AIMessage(content=\"no problem!\"),    HumanMessage(content=\"having fun?\"),    AIMessage(content=\"yes!\"),]trimmer.invoke(messages)API Reference:SystemMessage | trim_messages'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='[SystemMessage(content=\"you\\'re a good assistant\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'whats 2 + 2\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'4\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'thanks\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'no problem!\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'having fun?\\', additional_kwargs={}, response_metadata={}), AIMessage(content=\\'yes!\\', additional_kwargs={}, response_metadata={})]\\nTo  use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='To  use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\\nworkflow = StateGraph(state_schema=State)def call_model(state: State):    trimmed_messages = trimmer.invoke(state[\"messages\"])    prompt = prompt_template.invoke(        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}    )    response = model.invoke(prompt)    return {\"messages\": [response]}workflow.add_edge(START, \"model\")workflow.add_node(\"model\", call_model)memory = MemorySaver()app = workflow.compile(checkpointer=memory)\\nNow if we try asking the model our name, it won\\'t know it since we trimmed that part of the chat history:\\nconfig = {\"configurable\": {\"thread_id\": \"abc567\"}}query = \"What is my name?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='==================================\\x1b[1m Ai Message \\x1b[0m==================================I don\\'t know your name. You haven\\'t told me yet!\\nBut if we ask about information that is within the last few messages, it remembers:\\nconfig = {\"configurable\": {\"thread_id\": \"abc678\"}}query = \"What math problem did I ask?\"language = \"English\"input_messages = messages + [HumanMessage(query)]output = app.invoke(    {\"messages\": input_messages, \"language\": language},    config,)output[\"messages\"][-1].pretty_print()\\n==================================\\x1b[1m Ai Message \\x1b[0m==================================You asked what 2 + 2 equals.\\nIf you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.\\nStreaming‚Äã'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='If you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.\\nStreaming‚Äã\\nNow we\\'ve got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\\nIt\\'s actually super easy to do this!\\nBy default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='config = {\"configurable\": {\"thread_id\": \"abc789\"}}query = \"Hi I\\'m Todd, please tell me a joke.\"language = \"English\"input_messages = [HumanMessage(query)]for chunk, metadata in app.stream(    {\"messages\": input_messages, \"language\": language},    config,    stream_mode=\"messages\",):    if isinstance(chunk, AIMessage):  # Filter to just model responses        print(chunk.content, end=\"|\")\\n|Hi| Todd|!| Here|‚Äôs| a| joke| for| you|:|Why| don|‚Äôt| skeleton|s| fight| each| other|?|Because| they| don|‚Äôt| have| the| guts|!||\\nNext Steps‚Äã\\nNow that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='Conversational RAG: Enable a chatbot experience over an external source of data\\nAgents: Build a chatbot that can take actions\\n\\nIf you want to dive deeper on specifics, some things worth checking out are:\\n\\nStreaming: streaming is crucial for chat applications\\nHow to add message history: for a deeper dive into all things related to message history\\nHow to manage large message history: more techniques for managing a large chat history\\nLangGraph main docs: for more detail on building with LangGraph\\nEdit this pageWas this page helpful?PreviousBuild a simple LLM application with chat models and prompt templatesNextBuild a Retrieval Augmented Generation (RAG) App: Part 2OverviewSetupJupyter NotebookInstallationLangSmithQuickstartMessage persistencePrompt templatesManaging Conversation HistoryStreamingNext StepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Divide the data into chunks after getting it from source\n",
    "### Load Data--> Docs-->Divide our Documents into chunks documents-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1ae31702e70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstoredb = FAISS.from_documents(documents,embeddings)\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot's ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence‚Äã\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query = \"LangSmith Message persistence\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n        Answer the following question based only on the provided context:\\n            <context>\\n                {context}\\n            </context>\\n\\n\\n    '), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001AE10E64C50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001AE10D01790>, root_client=<openai.OpenAI object at 0x000001AE109219A0>, root_async_client=<openai.AsyncOpenAI object at 0x000001AE10E64CB0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## retrieval chain, document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## context here is the information I will be giving w.r.t. info of document or text to llm\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        Answer the following question based only on the provided context:\n",
    "            <context>\n",
    "                {context}\n",
    "            </context>\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangSmith Message persistence refers to the ability of LangSmith to store or maintain messages over time. However, the provided context does not include specific details about how this persistence is implemented or managed within LangSmith. If you have further details or a specific aspect you're interested in, please provide more information.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"LangSmith Message persistence\",\n",
    "    \"context\":[Document(page_content=\"LangSmith Message persistence\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input => Retriever => vectorstoredb\n",
    "from langchain.chains import create_retrieval_chain\n",
    "vectorstoredb\n",
    "\n",
    "retriever = vectorstoredb.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001AE31702E70>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n        Answer the following question based only on the provided context:\\n            <context>\\n                {context}\\n            </context>\\n\\n\\n    '), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001AE10E64C50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001AE10D01790>, root_client=<openai.OpenAI object at 0x000001AE109219A0>, root_async_client=<openai.AsyncOpenAI object at 0x000001AE10E64CB0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, the document discusses the implementation of a chatbot using LangGraph and focuses on features like message persistence and streaming to enhance user experience. The chatbot is designed to maintain conversational memory over multiple turns by utilizing LangGraph's persistence layer. It also mentions improving user experience through token streaming, allowing users to see progress in real-time as the language model generates responses.\\n\\nAdditionally, prompt templates are introduced as a way to structure raw user input into a format that a language model can process effectively. This involves adding system messages with custom instructions and considering additional input types.\\n\\nOverall, the main focus is on designing an LLM-powered chatbot that can remember previous interactions and improve with the use of structured input templates.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"LangSmith Message persistence\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmith Message persistence',\n",
       " 'context': [Document(id='12b5eb04-a768-47de-a717-df75a7eae613', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"And now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot's ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence‚Äã\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\"),\n",
       "  Document(id='a6b63743-7eaa-4536-bb4b-d6931c2e3ad5', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='If you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.\\nStreaming‚Äã\\nNow we\\'ve got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\\nIt\\'s actually super easy to do this!\\nBy default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:'),\n",
       "  Document(id='6d1e74b6-097c-4d77-9d3f-8586b8dbb635', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"noteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nOverview‚Äã\\nWe'll go over an example of how to design and implement an LLM-powered chatbot.\\nThis chatbot will be able to have a conversation and remember previous interactions with a chat model.\\nNote that this chatbot that we build will only use the language model to have a conversation.\"),\n",
       "  Document(id='fd8b4bbb-f0b7-416d-bfca-b897c1d115c2', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"Right now, all we've done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\\nPrompt templates‚Äã\\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\\nTo add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\")],\n",
       " 'answer': \"Based on the provided context, the document discusses the implementation of a chatbot using LangGraph and focuses on features like message persistence and streaming to enhance user experience. The chatbot is designed to maintain conversational memory over multiple turns by utilizing LangGraph's persistence layer. It also mentions improving user experience through token streaming, allowing users to see progress in real-time as the language model generates responses.\\n\\nAdditionally, prompt templates are introduced as a way to structure raw user input into a format that a language model can process effectively. This involves adding system messages with custom instructions and considering additional input types.\\n\\nOverall, the main focus is on designing an LLM-powered chatbot that can remember previous interactions and improve with the use of structured input templates.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='12b5eb04-a768-47de-a717-df75a7eae613', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"And now we can see that we get a good response!\\nThis is the basic idea underpinning a chatbot's ability to interact conversationally.\\nSo how do we best implement this?\\nMessage persistence‚Äã\\nLangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\"),\n",
       " Document(id='a6b63743-7eaa-4536-bb4b-d6931c2e3ad5', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content='If you take a look at LangSmith, you can see exactly what is happening under the hood in the LangSmith trace.\\nStreaming‚Äã\\nNow we\\'ve got a functioning chatbot. However, one really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\\nIt\\'s actually super easy to do this!\\nBy default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:'),\n",
       " Document(id='6d1e74b6-097c-4d77-9d3f-8586b8dbb635', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"noteThis tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.If your code is already relying on RunnableWithMessageHistory or BaseChatMessageHistory, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses RunnableWithMessageHistory will continue to work as expected.Please see How to migrate to LangGraph Memory for more details.\\nOverview‚Äã\\nWe'll go over an example of how to design and implement an LLM-powered chatbot.\\nThis chatbot will be able to have a conversation and remember previous interactions with a chat model.\\nNote that this chatbot that we build will only use the language model to have a conversation.\"),\n",
       " Document(id='fd8b4bbb-f0b7-416d-bfca-b897c1d115c2', metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a Chatbot | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.', 'language': 'en'}, page_content=\"Right now, all we've done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.\\nPrompt templates‚Äã\\nPrompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\\nTo add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
