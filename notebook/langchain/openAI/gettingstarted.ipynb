{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001F97BADDF40> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F97BB043E0> root_client=<openai.OpenAI object at 0x000001F979489820> root_async_client=<openai.AsyncOpenAI object at 0x000001F97BADDFA0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Catastrophic forgetting, also known as catastrophic interference, is a phenomenon in the field of machine learning and neural networks where a model or artificial neural network, after learning new information or tasks, loses or overwrites previously acquired knowledge. This often occurs during sequential learning, where the model learns multiple tasks one after the other.\\n\\nWhen a neural network is trained on a new task, the weights and parameters of the network are updated to optimize performance on that task. However, these updates can interfere destructively with the learned weights associated with previous tasks, leading to significant drops in performance on those earlier tasks. This challenge is particularly pronounced in continual learning and lifelong learning scenarios where the goal is for the model to retain knowledge across various tasks over time.\\n\\nAddressing catastrophic forgetting is an active area of research. Potential solutions include:\\n\\n1. **Regularization techniques**: These involve adding constraints or penalties to the loss function to discourage the model from making significant changes to weights that are important for previous tasks. Examples include elastic weight consolidation (EWC) and synaptic intelligence.\\n\\n2. **Memory-based approaches**: These involve storing and replaying examples from previous tasks to help the network remember past information. Techniques include rehearsal methods and generative replay.\\n\\n3. **Architectural solutions**: These aim to change the structure of the network to accommodate learning new tasks without interfering with older ones. Examples include dynamically expandable networks and using distinct subnetworks for different tasks.\\n\\n4. **Meta-learning approaches**: By training the model on how to learn, these methods help improve a neural network's ability to generalize across tasks without forgetting.\\n\\nCatastrophic forgetting highlights the challenge of creating AI that can learn cumulatively and adaptively like human learning, and finding effective ways to mitigate it remains a crucial endeavor in advancing AI capabilities.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 361, 'prompt_tokens': 12, 'total_tokens': 373, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6ec83003ad', 'id': 'chatcmpl-BCk1RZ8yzwOKMgQES774PxVPexd05', 'finish_reason': 'stop', 'logprobs': None} id='run-67624479-6d6a-469b-8b93-13382198edb0-0' usage_metadata={'input_tokens': 12, 'output_tokens': 361, 'total_tokens': 373, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is catastrophic forgetting?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## how llm role or will behave\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer.Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Certainly! If you have multiple prompt templates, it's important to manage and utilize them effectively, especially in contexts such as using AI chatbots or language models. Here's how you can handle and use multiple prompt templates:\\n\\n1. **Organization**:\\n   - **Categorize**: Group prompts based on their function, such as informational, creative, instructional, etc.\\n   - **Labeling**: Clearly label each template for easy identification. Use descriptive names that explain the purpose of the prompt.\\n\\n2. **Selection Strategy**:\\n   - **Contextual Matching**: Choose a prompt template that best suits the context or intent of the userâ€™s query or task.\\n   - **Priority System**: Determine if certain prompts should take precedence over others and establish rules for this prioritization.\\n\\n3. **Dynamic Switching**:\\n   - Implement a system that can dynamically switch between templates based on real-time user input or interactions. This could involve setting conditions or triggers that change the prompt based on user responses or predefined criteria.\\n\\n4. **Testing and Iteration**:\\n   - Continuously test the effectiveness of each template in different scenarios. Collect feedback and make necessary adjustments or improvements.\\n   - A/B testing can help determine which templates perform better for specific tasks.\\n\\n5. **Personalization**:\\n   - Incorporate user preferences or historical interactions to personalize prompt selection. This could involve using machine learning models to predict the most suitable prompts for individual users.\\n\\n6. **Fallbacks**:\\n   - Have default or fallback templates for situations where specific prompts do not apply or when the system is uncertain about which prompt to use.\\n\\n7. **Documentation and Guidelines**:\\n   - Maintain comprehensive documentation of all your prompt templates. Include guidelines on their intended usage, limitations, and examples of inputs they handle well.\\n\\nIf you are integrating these templates into a system for users, ensure that it provides a seamless experience, adapting to their needs while maintaining clarity in communication.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 389, 'prompt_tokens': 41, 'total_tokens': 430, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6ec83003ad', 'id': 'chatcmpl-BCkCIJJq8zliwmgVPXe6aJHL6tbf0', 'finish_reason': 'stop', 'logprobs': None} id='run-f3d2ee0a-92a9-4348-a77d-7fec1ea39815-0' usage_metadata={'input_tokens': 41, 'output_tokens': 389, 'total_tokens': 430, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "## create chain\n",
    "\n",
    "chain = prompt|llm\n",
    "response = chain.invoke({\"input\":\"Can you tell me if i have multiple prompt template and how to use them?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can certainly have multiple prompt templates, especially if you're working with different tasks or contexts within AI applications like language models. Here's how you can utilize them effectively:\n",
      "\n",
      "1. **Define Clear Objectives:** Identify specific objectives for each prompt template. This might include generating creative content, summarizing texts, answering questions, or performing code reviews.\n",
      "\n",
      "2. **Design Templates for Specific Contexts:** Customize templates based on the context or the type of task. Consider the required tone, length, and style of the response. For example, you might have a template for formal emails and another for casual conversation.\n",
      "\n",
      "3. **Incorporate Variables:** Use placeholders or variables within your templates to insert dynamic content. For example, `Dear {Name},` or `Summarize the following text: {Content}`. This allows for flexibility and reusability of templates.\n",
      "\n",
      "4. **Test and Iterate:** Once you've created your templates, test them with multiple inputs to ensure they produce desired outcomes. Iteratively refine the templates based on feedback and results.\n",
      "\n",
      "5. **Organize and Document:** Maintain a well-organized system for storing and documenting your templates. Tools like template libraries or content management systems can help manage multiple templates efficiently.\n",
      "\n",
      "6. **Automate Usage:** Integrate the templates into your application flow seamlessly. Use scripting or API calls to select and populate templates dynamically based on specific criteria or user inputs.\n",
      "\n",
      "By strategically using multiple prompt templates, you can enhance the functionality and adaptability of your AI applications, ensuring they meet diverse user needs and contexts effectively.\n"
     ]
    }
   ],
   "source": [
    "## parsing the output using stroutput Parser\n",
    "## we can also create the output parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser\n",
    "response = chain.invoke({\"input\":\"Can you tell me if i have multiple prompt template and how to use them?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
